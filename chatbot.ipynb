{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atxc4PbA3AOJ",
        "outputId": "55918c05-80b1-4596-f803-e41ec5390fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting the Google Drive\n",
        "# eta r theke = https://www.projectpro.io/article/python-chatbot-project-learn-to-build-a-chatbot-from-scratch/429\n",
        "from google. colab import drive\n",
        "drive. mount( '/content/drive' )\n",
        "data_root = '/content/drive/MyDrive/chatbot'\n",
        "#P1ease upload the files in your drive and change the pat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download (\"wordnet\")"
      ],
      "metadata": {
        "id": "keaitmbm4HEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865125c5-c030-4d05-868c-7ebbc5e52780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "data_files = glob.glob('/content/drive/MyDrive/chatbot/*.json')\n",
        "\n",
        "for data_file in data_files:\n",
        "  data = json.loads(open(data_file).read())\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "Piogfb2J6Hyy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Creating data_X and data_Y\n",
        "words =[]\n",
        "classes=[]\n",
        "data_x = []\n",
        "data_y = []\n",
        "\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "      tokens = nltk.word_tokenize(pattern)\n",
        "      words.extend(tokens)\n",
        "      data_x.append(pattern)\n",
        "      data_y.append(intent[\"tag\"])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "# sorting the vocab and classes in alphabetical order and # set to ensure no duplicates occur\n",
        "\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "jXpiuPAO7y2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(classes))\n",
        "print(data_y[idx])\n",
        "if data_y[idx] not in classes:\n",
        "  print(\"Error: data_y[idx] is not in classes\")\n",
        "  output_row[min(max(classes.index(data_y[idx]), 0), len(classes) - 1)] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecVcI2DR2uTs",
        "outputId": "aa9f85c2-a17d-474c-f0ab-6d54c6c6a9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "classes = sorted(set(data_y))\n",
        "#creating the bag of words model\n",
        "\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "\n",
        "  output_row = list(out_empty)\n",
        "  output_row[classes.index(data_y[idx])] = 1\n",
        "\n",
        "  training.append([bow, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype = object)\n",
        "\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_Y = np.array(list(training[:, 1]))"
      ],
      "metadata": {
        "id": "w0Ld_zBmBO-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 The neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128,input_shape=(len(train_X[0]),),activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation = 'relu' ))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_Y[0]), activation = \"Softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "def model_compile(loss, optimizer, metrics):\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "model_compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(x = train_X, y = train_Y, epochs=150, verbose = 1)"
      ],
      "metadata": {
        "id": "pwQ-_z9PEux_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "  tokens=[lemmatizer.lemmatize(word)for word in tokens]\n",
        "  return tokens\n",
        "def bag_of_words(text,vocab):\n",
        "  tokens=clean_text(text)\n",
        "  bow=[0]*len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx,word in enumerate(vocab):\n",
        "      if word==w:\n",
        "        bow[idx]=1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text,vocab,labels):\n",
        "  bow=bag_of_words(text,vocab)\n",
        "  result=model.predict(np.array([bow]))[0]\n",
        "  thresh=0.5\n",
        "  y_pred=[[indx,res] for indx,res in enumerate(result) if res>thresh]\n",
        "  y_pred.sort(key=lambda x:x[1],reverse=True)\n",
        "  return_list=[]\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]])\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list,intents_json):\n",
        "  if len(intents_list)==0:\n",
        "    result=\"Sorry! I don't understand\"\n",
        "  else:\n",
        "    tag=intents_list[0]\n",
        "    list_of_intents=intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"]==tag:\n",
        "        result=random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result"
      ],
      "metadata": {
        "id": "po-IUecfKLmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_class(sentence, words, classes):\n",
        "    # Function to predict the class of the sentence\n",
        "\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    bag = [1 if word in words else 0 for word in sentence_words]\n",
        "    return np.array(bag)\n",
        "\n",
        "def get_response(intents_list, data):\n",
        "    # Function to get the bot's response\n",
        "\n",
        "    tag = intents_list[0]['intent']\n",
        "    list_of_intents = data['intents']\n",
        "    result = \"\"\n",
        "    for i in list_of_intents:\n",
        "        if i['intent'] == tag:\n",
        "            result = i['response']\n",
        "            break\n",
        "    return result"
      ],
      "metadata": {
        "id": "0LnTF_n60Z-a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Press 0 if you don't want to chat with our ChatBot.\")\n",
        "while True:\n",
        "  message=input(\"\")\n",
        "  if message==\"0\":\n",
        "    break\n",
        "  intents=pred_class(message,words,classes)\n",
        "  result=get_response(intents,data)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbVSPlm7BqYB",
        "outputId": "63735fe5-43c7-4fb2-c2ed-1ecb7cdeebd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Press 0 if you don't want to chat with our ChatBot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGXLof5IDcBE"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}